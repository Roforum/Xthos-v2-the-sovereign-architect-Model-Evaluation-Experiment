# Xthos-v2-the-sovereign-architect Model Evaluation Experiment

## Overview

This repository documents an experiment evaluating the performance of the fine-tuned AI model [AiAsistent/xthos-v2-the-sovereign-architect](https://huggingface.co/AiAsistent/xthos-v2-the-sovereign-architect), a 4B parameter model, against several leading large language models (LLMs) in the task of generating synthetic data. The experiment highlights the model's capabilities after fine-tuning on 100 million tokens using LoRA with parameters r=256 and alpha=512.

While official benchmarks may show lower scores compared to the base model, this test demonstrates that the fine-tuned model can outperform some larger counterparts in specific scenarios. Note that all evaluations are inherently subjective, as they depend on the proprietary prompt and analysis criteria. The purpose of sharing this repository is to encourage others to replicate, test, and analyze the results independently, fostering a more objective community-driven perspective.

The fine-tuning method and the exact prompt used for synthetic data generation (consisting of a system prompt and JSON-formatted text) are proprietary. I have chosen not to disclose them to protect my intellectual property and innovative techniques in data synthesis. However, sufficient details are provided here to allow for meaningful replication and extension of the experiment.

**Author:** Alexh  
**Date:** January 08, 2026  

## Quick Links

- **Original Model (Full Weights):** [AiAsistent/xthos-v2-the-sovereign-architect](https://huggingface.co/AiAsistent/xthos-v2-the-sovereign-architect)
- **Ollama Version:** [aiasistentworld/xthos-v2](https://ollama.com/library/aiasistentworld/xthos-v2) (Run with: `ollama run aiasistentworld/xthos-v2`)
- **Official Documentation & Research:** [LLMResearch.net](https://llmresearch.net) (Owned and operated by the author)

## Models Tested

The following models were evaluated in a side-by-side comparison:

1. **Model 1:** AiAsistent/xthos-v2-the-sovereign-architect (via Ollama with Python) â€“ The fine-tuned 4B model under evaluation.
2. **Model 2:** z.ai GLM 4.7 (web-based chat).
3. **Model 3:** Claude (web-based chat).
4. **Model 4:** ChatGPT (web-based chat).
5. **Model 5:** Qwen AI (web-based chat).
6. **Model 6:** DeepSeek (web-based chat).
7. **Model 7:** Grok 4.1 (web-based chat).
8. **Model 8:** Kimi 2 (web-based chat).
9. **Model 9:** Gemini 3 (in Google AI Studio).

All models received the same proprietary prompt to generate synthetic data. Models 3, 4, and 5 required a second attempt due to initial misunderstandings, refusals, or requests for clarification.

## Methodology

### Phase 1: Synthetic Data Generation
- Each model was provided with the identical proprietary prompt (system prompt + JSON-formatted text) to generate synthetic data.
- Responses were collected and saved in a delimited file, with sections marked for each model (e.g., "Model 1 Response", etc.).
- The primary focus was assessing Model 1's performance relative to larger models in this task.

### Phase 2: Self-Analysis by Models
- The aggregated responses from Phase 1 were fed back to each model for analysis using the following task prompt:
  ```
  You are a recognized specialist in synthetic data analysis.
  Your task is to analyze the synthetic data generated by each model and then provide a complete report.
  - Which model was the best
  - Which model was the weakest
  - Which model was the best at the 'human level'
  - Which model caught your attention the most and why
  - If you were training a top AI model right now, which model would you choose and why?
  - Other data and honest personal opinion.
  At the end, make a top with all the models, the first being the best in descending order and the percentage from 1 to 100 for each.
  ```
- This serves as a baseline for self-evaluation. Users replicating the experiment should substitute their own generation results here to perform similar analyses.

### Phase 3: Final Aggregated Analysis
- Analysis responses from Phase 2 were compiled into a single delimited file.
- This file was then analyzed using Gemini 3 in Google AI Studio to generate an overall ranking and aggregated scores.
- The multi-phase design aims to illustrate the fine-tuned model's strengths, emphasizing the success of the proprietary fine-tuning approach.

## Results

Detailed results, including raw responses from data generation, individual model analyses, and the final aggregated report, are attached as files in this repository. Please review them for specifics.

Key subjective observations from the experiment:
- Model 1 (the fine-tuned 4B model) demonstrated strong performance, often surpassing larger models in creativity, coherence, and "human-level" quality.
- This validates the effectiveness of fine-tuning smaller models for niche tasks like synthetic data generation.

As noted, these findings are subjective. I encourage community members to run their own tests with custom prompts and share insights to build a more comprehensive, objective understanding.

## How to Replicate

To replicate this experiment:
1. Install Ollama and pull the model: `ollama pull aiasistentworld/xthos-v2`.
2. Interact with Model 1 either via Python (using the Ollama API) or through the Ollama UI application. From personal testing, the UI tends to yield slightly better responses, but results may vary.
3. Access the web-based interfaces for the other models listed.
4. Develop your own prompt structure (system prompt + JSON text) for synthetic data generation, as the original is proprietary.
5. Follow the methodology phases: Generate data, perform self-analyses with your results, and aggregate using a tool like Gemini 3.
6. Compare outcomes to assess Model 1's performance in your context.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Gratitude to the Hugging Face and Ollama communities for model hosting and tools.
- This work draws from ongoing research at [LLMResearch.net](https://llmresearch.net), where further explorations into LLM fine-tuning and evaluation are documented.
